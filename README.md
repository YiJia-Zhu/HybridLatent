<!-- <p align="center" width="100%">
<img src="./docs/static/images/logo_resize.png"  width="80%">
</p> -->

<div align="center">
    <h1 align="center"> SIM-CoT: Supervised Implicit Chain-of-Thought
    </h1>
</div>

<p align="center">
  <img src="assets/coconut_teaser.png">
</p>


- **Authors**: [Xilin Wei](https://github.com/Wiselnn570), [Xiaoran Liu](https://scholar.google.de/citations?user=Qe6F4J4AAAAJ&hl=en), [Yuhang Zang](https://yuhangzang.github.io), [Xiaoyi Dong](https://lightdxy.github.io), [Yuhang Cao](https://scholar.google.com/citations?user=sJkqsqkAAAAJ&hl=en), [Jiaqi Wang](https://myownskyw7.github.io/), [Xipeng Qiu](https://xpqiu.github.io/en.html), [Dahua Lin](http://dahua.site/)
- **Institutes**: Fudan University; Shanghai AI Laboratory; The Chinese University of Hong Kong; Shanghai Innovation Institute; 
- **Resources**: [ğŸ“–[Paper](https://arxiv.org/pdf/2509.20317)] [[ğŸ Project Page]()] [[ğŸ¤—Huggingface](https://huggingface.co/collections/Wiselnn/sim-cot-supervised-implicit-chain-of-thought-68d895b00576f6166c19ab4f)]
## ğŸ’¡ Highlights

- ğŸ”¥ **Latent Instability in Implicit CoT:** We systematically analyze the limitations of implicit Chain-of-Thought methods and reveal a **latent instability issue**â€”as the number of implicit tokens increases, models tend to collapse into homogeneous latent states that lose operator semantics.  

- ğŸ”¥ **Step-Level Supervision with SIM-CoT:** We propose **S**upervised **IM**plicit-CoT (**SIM-CoT**), a plug-and-play module that introduces **step-level supervision** via an auxiliary decoder. This stabilizes optimization, prevents collapse, and ensures that latent tokens capture meaningful reasoning steps.

- ğŸ”¥ **Strong and Consistent Performance:** SIM-CoT consistently outperforms both explicit and implicit baselines. On GPT-2, it exceeds supervised CoT by **+2.1%**, Coconut by **+8.2%**, and CODI by **+4.3%**. Across larger LLaMA models (1B/3B/8B), it delivers **+1.5% to +9.0%** gains, and remains stable even with **8â€“16 implicit tokens**, where prior methods collapse.  

- ğŸ”¥ **Efficiency and Interpretability:** SIM-CoT adds **no extra inference cost** since the auxiliary decoder is discarded after training. It also provides **interpretability**, allowing each latent token to be decoded into a human-readable reasoning step.  

## ğŸ“œ News

**[2025/9/24]** [Code]() and [Paper](https://arxiv.org/pdf/2509.20317) are released!

## ğŸ‘¨â€ğŸ’» Todo

- [x] Code Release
- [x] Checkpoint Release
- [x] Usage Instructions Release


## ğŸ› ï¸ Usage

### 1. Clone the repository
```bash
git clone https://github.com/InternLM/SIM-CoT.git
cd SIM-CoT
```

### 2. Install dependencies
```bash
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

---
export CUDA_VISIBLE_DEVICES="4,5,6,7"
### 3. Training with Coconut + SIM-CoT

#### Step 1: Train the Coconut baseline
```bash
cd Coconut
torchrun --nnodes 1 --nproc_per_node 4 run.py args/gsm_coconut.yaml
```

#### Step 2: Continue training with SIM-CoT
Select a checkpoint that has been expanded to predefined implicit tokens, then continue training with SIM-CoT:
```bash
torchrun --nnodes 1 --nproc_per_node 4 run.py args/gsm_simcot.yaml
```

---

### 4. Evaluation with Coconut + SIM-CoT
```bash
torchrun --nnodes 1 --nproc_per_node 4 run.py args/gsm_simcot_eval.yaml
```

---

### 5. Training with CODI + SIM-CoT
```bash
cd CODI
bash scripts/train_llama3b_gsm8k-aug-decoder-2.sh
```

---

### 6. Evaluation with CODI + SIM-CoT
```bash
bash CODI/scripts/test_llama3b-copy.sh
```

æ ¹æ®æˆ‘ä» CODI é¡¹ç›® GitHub å’Œè®ºæ–‡ä¸­è·å–çš„ä¿¡æ¯ï¼Œè®©æˆ‘ä¸ºä½ è¯¦ç»†è§£é‡Šè¿™ä¸ªè®­ç»ƒè„šæœ¬ä¸­æ¯ä¸ªå‚æ•°çš„å«ä¹‰ï¼š

## ğŸ“ åŸºç¡€è·¯å¾„ä¸ç¯å¢ƒè®¾ç½®

| å‚æ•° | å€¼ | å«ä¹‰ |
|------|-----|------|
| `SAVE_DIR` | `./codi_llama1b_full` | æ¨¡å‹ä¿å­˜ç›®å½• |
| `CUDA_VISIBLE_DEVICES` | `4,5,6,7` | ä½¿ç”¨çš„ GPU ç¼–å· |

## ğŸ”§ Hugging Face Trainer æ ‡å‡†å‚æ•°

| å‚æ•° | å€¼ | å«ä¹‰ |
|------|-----|------|
| `--output_dir` | `$SAVE_DIR` | è¾“å‡ºç›®å½• |
| `--expt_name` | `gsm8k_llama1b_latent_baseline` | å®éªŒåç§° |
| `--logging_dir` | `$SAVE_DIR/logs` | TensorBoard æ—¥å¿—ç›®å½• |
| `--logging_steps` | `10` | æ¯ 10 æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿— |
| `--model_name_or_path` | `./pretrained/Llama-3.2-1B-Instruct` | é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ |
| `--data_name` | `icot` | æ•°æ®é›†åç§° (implicit CoT) |
| `--seed` | `11` | éšæœºç§å­ |
| `--model_max_length` | `512` | æœ€å¤§åºåˆ—é•¿åº¦ |
| `--per_device_train_batch_size` | `16` | æ¯ä¸ª GPU çš„ batch size |
| `--gradient_accumulation_steps` | `4` | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° (æœ‰æ•ˆ batch = 16Ã—4Ã—4GPU = 256) |
| `--bf16` | - | ä½¿ç”¨ BFloat16 æ··åˆç²¾åº¦è®­ç»ƒ |
| `--num_train_epochs` | `10` | è®­ç»ƒè½®æ•° |
| `--learning_rate` | `8e-4` | å­¦ä¹ ç‡ |
| `--max_grad_norm` | `2.0` | æ¢¯åº¦è£å‰ªé˜ˆå€¼ |
| `--save_strategy` | `no` | ä¸ä¿å­˜ä¸­é—´ checkpoint |
| `--save_total_limit` | `1` | æœ€å¤šä¿ç•™ 1 ä¸ª checkpoint |
| `--save_safetensors` | `False` | ä¸ä½¿ç”¨ safetensors æ ¼å¼ |
| `--weight_decay` | `0.1` | æƒé‡è¡°å‡ |
| `--warmup_ratio` | `0.03` | é¢„çƒ­æ¯”ä¾‹ (3% çš„è®­ç»ƒæ­¥æ•°) |
| `--lr_scheduler_type` | `cosine` | ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦ |
| `--do_train` | - | æ‰§è¡Œè®­ç»ƒ |
| `--report_to` | `tensorboard` | æ—¥å¿—æŠ¥å‘Šåˆ° TensorBoard |
| `--logging_strategy` | `steps` | æŒ‰æ­¥æ•°è®°å½•æ—¥å¿— |

## ğŸ¯ LoRA ç›¸å…³å‚æ•°

| å‚æ•° | å€¼ | å«ä¹‰ |
|------|-----|------|
| `--use_lora` | `True` | å¯ç”¨ LoRA å¾®è°ƒ |
| `--lora_r` | `128` | LoRA çš„ç§© (rank)ï¼Œè¶Šå¤§è¡¨è¾¾èƒ½åŠ›è¶Šå¼º |
| `--lora_alpha` | `32` | LoRA ç¼©æ”¾å› å­ |
| `--lora_init` | - | ä½¿ç”¨ç‰¹æ®Šçš„ LoRA åˆå§‹åŒ– |

## ğŸ§  CODI æ ¸å¿ƒå‚æ•°

| å‚æ•° | å€¼ | å«ä¹‰ |
|------|-----|------|
| `--num_latent` | `6` | è®­ç»ƒæ—¶ä½¿ç”¨çš„éšå¼æ€ç»´ token æ•°é‡ |
| `--use_prj` | `True` | æ˜¯å¦å¯¹æœ€åä¸€å±‚ hidden state ä½¿ç”¨æŠ•å½±å±‚ |
| `--prj_dim` | `2048` | æŠ•å½±å±‚çš„éšè—ç»´åº¦ |
| `--prj_dropout` | `0.0` | æŠ•å½±å±‚çš„ dropout ç‡ |
| `--distill_loss_div_std` | `True` | æ˜¯å¦ç”¨ teacher hidden state çš„æ ‡å‡†å·®æ¥å½’ä¸€åŒ–è’¸é¦æŸå¤± |
| `--distill_loss_factor` | `20` | è’¸é¦æŸå¤±çš„æƒé‡ç³»æ•° |
| `--max_token_num` | `200` | ä¸¢å¼ƒè¶…è¿‡æ­¤ token é•¿åº¦çš„è®­ç»ƒæ ·æœ¬ |
| `--remove_eos` | `True` | ç§»é™¤ EOS token |
| `--print_ref_model_stats` | `True` | æ‰“å°å‚è€ƒæ¨¡å‹çš„ç»Ÿè®¡ä¿¡æ¯ |

## ğŸ”¬ å®éªŒæ¨¡å¼å‚æ•°

| å‚æ•° | å€¼ | å«ä¹‰ |
|------|-----|------|
| `--exp_mode` | `False` | æ˜¯å¦ä¸ºå®éªŒæ¨¡å¼ (ç”¨äºå¿«é€Ÿè°ƒè¯•) |
| `--exp_data_num` | `200` | å®éªŒæ¨¡å¼ä¸‹ä½¿ç”¨çš„æ•°æ®é‡ |

## ğŸ“– CODI æ¡†æ¶ç®€ä»‹

CODI æ˜¯ä¸€ä¸ªè‡ªè’¸é¦æ¡†æ¶ï¼ŒåŒ…å« teacher ä»»åŠ¡å’Œ student ä»»åŠ¡ã€‚Teacher ä»»åŠ¡å­¦ä¹ æ˜¾å¼ CoT æ¨ç†ï¼Œstudent ä»»åŠ¡å­¦ä¹ éšå¼ CoT æ¨ç†ã€‚çŸ¥è¯†è’¸é¦é€šè¿‡å¯¹é½å…³é”® token çš„ hidden activation æ¥å®ç°ã€‚

æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
- **Teacher**ï¼šä½¿ç”¨æ ‡å‡†çš„ Chain-of-Thoughtï¼ˆæ˜¾å¼æ¨ç†æ­¥éª¤ï¼‰
- **Student**ï¼šä½¿ç”¨è¿ç»­ç©ºé—´ä¸­çš„éšå¼æ€ç»´ tokenï¼ˆ`num_latent=6` ä¸ªï¼‰
- **è’¸é¦**ï¼šé€šè¿‡å¯¹é½ä¸¤è€…åœ¨ç”Ÿæˆç­”æ¡ˆä½ç½®çš„ hidden state æ¥ä¼ é€’çŸ¥è¯†

## âœ’ï¸ Citation

If you find our work helpful for your research, please consider giving a star â­ and citation ğŸ“

```bibtex
@article{wei2025simcot,
  title={{SIM-COT}: Supervised Implicit Chain-of-Thought},
  author={Wei, Xilin and Liu, Xiaoran and Zang, Yuhang and Dong, Xiaoyi and Cao, Yuhang and Wang, Jiaqi and Qiu, Xipeng and Lin, Dahua},
  journal={arXiv preprint arXiv:2509.20317},
  year={2025}
}
```

## â¤ï¸ Acknowledgments

- [Coconut](https://github.com/facebookresearch/coconut): The codebase we built upon. Thanks for their wonderful work.
- [CODI](https://github.com/zhenyi4/codi): Our work is based on this codebase; we are grateful for their valuable contribution.
- [LLaMA series](https://huggingface.co/meta-llama/collections): The amazing open-sourced large language model!
- [GPT2](https://huggingface.co/openai-community/gpt2): An impressive open-source large language model!


